{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6170897-217d-48a3-a8a0-c7f296e3c290",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84f1734e-5a08-4309-8ae1-8703f252cbe7",
   "metadata": {},
   "source": [
    "# DCGAN and CGAN Image Generation\n",
    "\n",
    "This notebook demonstrates the implementation and training of Deep Convolutional Generative Adversarial Networks (DCGAN) and Conditional GANs (CGAN) for image generation. The notebook is organized into several sections that cover data preparation, model architecture, training, and evaluation.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup and Imports](#Setup-and-Imports)\n",
    "3. [Data Loading and Preprocessing](#Data-Loading-and-Preprocessing)\n",
    "4. [Model Architecture](#Model-Architecture)\n",
    "5. [Training the Models](#Training-the-Models)\n",
    "6. [Evaluation and Visualization](#Evaluation-and-Visualization)\n",
    "7. [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeebb3e-a3cf-4589-beef-3d6af501666c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will explore the use of DCGAN and CGAN models for generating images. DCGAN is a popular GAN architecture that uses convolutional networks, while CGAN extends this by conditioning the generation process on additional information, such as class labels. We will train these models on a dataset of images and evaluate their performance.\n",
    "\n",
    "The key objectives are:\n",
    "- Implement and train DCGAN and CGAN models.\n",
    "- Compare the effectiveness of conditioned vs. unconditioned image generation.\n",
    "- Visualize and analyze the quality of generated images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e911e-c6f7-4b53-bc1c-2f0b4182bcd2",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d5e2b-742f-4e2a-8094-b8567c677011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for deep learning and image processing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "LEARNING_RATE = 2e-4  # Learning rate for both Generator and Discriminator\n",
    "BATCH_SIZE = 128  # Batch size for training\n",
    "IMAGE_SIZE = 64  # Image size (64x64)\n",
    "CHANNELS_IMG = 3  # Number of channels in the input image (3 for RGB)\n",
    "NOISE_DIM = 100  # Dimension of the noise vector\n",
    "NUM_EPOCHS = 5000  # Number of epochs to train\n",
    "FEATURES_DISC = 64  # Base number of features in the Discriminator\n",
    "FEATURES_GEN = 64  # Base number of features in the Generator\n",
    "LOG_DIR_REAL = \"logs/real\"  # Directory for TensorBoard logs (real images)\n",
    "LOG_DIR_FAKE = \"logs/fake\"  # Directory for TensorBoard logs (fake images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0686dcc-cb35-4bb0-844b-36a6a064bcba",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e70ba4d-3daa-42e0-bdac-a33aa00330de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing function\n",
    "def load_data(batch_size=128, image_size=64):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the dataset for GAN training.\n",
    "    \n",
    "    Args:\n",
    "    - batch_size (int): The number of samples per batch.\n",
    "    - image_size (int): The size of the images after resizing.\n",
    "    \n",
    "    Returns:\n",
    "    - DataLoader: The DataLoader object for the dataset.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)])\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.ImageFolder(root=\"/content/drive/MyDrive/THESIS_GAN_TRAINING_FILES/141228_combined/\", transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# Load the dataset\n",
    "dataloader = load_data(batch_size=BATCH_SIZE, image_size=IMAGE_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a4b27-3c99-4821-8a73-c6da5ce216fc",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec2e06-873a-4c4d-9fa8-715c0fa1749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        \"\"\"\n",
    "        Initializes the Discriminator model.\n",
    "        \n",
    "        Args:\n",
    "        - channels_img (int): Number of image channels (e.g., 3 for RGB).\n",
    "        - features_d (int): Base number of features in the Discriminator.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \"\"\"Returns a block of Conv2d -> BatchNorm2d -> LeakyReLU.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "# Define the Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, channels_img, features_g):\n",
    "        \"\"\"\n",
    "        Initializes the Generator model.\n",
    "        \n",
    "        Args:\n",
    "        - noise_dim (int): Dimension of the input noise vector.\n",
    "        - channels_img (int): Number of image channels (e.g., 3 for RGB).\n",
    "        - features_g (int): Base number of features in the Generator.\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            self._block(noise_dim, features_g * 16, 4, 1, 0),\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),\n",
    "            nn.ConvTranspose2d(features_g * 2, channels_img, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        \"\"\"Returns a block of ConvTranspose2d -> BatchNorm2d -> ReLU.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfed94f-cf2a-49e4-8eba-cdc57e0f5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights of the models\n",
    "def initialize_weights(model):\n",
    "    \"\"\"\n",
    "    Initializes the weights of the model based on the DCGAN paper.\n",
    "    \n",
    "    Args:\n",
    "    - model (nn.Module): The model to initialize weights for.\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "# Test the Generator and Discriminator models\n",
    "def test_models():\n",
    "    \"\"\"Tests the Generator and Discriminator architectures.\"\"\"\n",
    "    N, in_channels, H, W = 8, 3, 64, 64\n",
    "    noise_dim = 100\n",
    "    x = torch.randn((N, in_channels, H, W))\n",
    "    disc = Discriminator(in_channels, 8)\n",
    "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
    "    \n",
    "    gen = Generator(noise_dim, in_channels, 8)\n",
    "    z = torch.randn((N, noise_dim, 1, 1))\n",
    "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n",
    "    \n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbed28d-ab9f-47dc-8892-0d12ebfa97de",
   "metadata": {},
   "source": [
    "## Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4481d9f-3659-480e-9f4a-17d05d79c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)\n",
    "\n",
    "# Print the model architectures\n",
    "print(\"Generator Architecture:\\n\", gen)\n",
    "print(\"\\nDiscriminator Architecture:\\n\", disc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab31a6-801f-446f-8b0c-ce739d750059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)\n",
    "\n",
    "# Print the model architectures\n",
    "print(\"Generator Architecture:\\n\", gen)\n",
    "print(\"\\nDiscriminator Architecture:\\n\", disc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbdad21-89e6-42a4-8ab4-b9556e479c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard setup\n",
    "fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(LOG_DIR_REAL)\n",
    "writer_fake = SummaryWriter(LOG_DIR_FAKE)\n",
    "step = 0\n",
    "\n",
    "# Training loop\n",
    "gen.train()\n",
    "disc.train()\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real, _) in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.size(0)\n",
    "        \n",
    "        # Generate fake images\n",
    "        noise = torch.randn(cur_batch_size, NOISE_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        disc_real = disc(real).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        \n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        output = disc(fake).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        \n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        # Logging and visualization\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} Loss D: {loss_disc:.4f}, Loss G: {loss_gen:.4f}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "            \n",
    "            step += 1\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Training Time: {(end - begin)/60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f521b-1688-4723-93e3-c202d8b695ed",
   "metadata": {},
   "source": [
    "## Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b10a33b3-49a7-4c84-9f5c-7747c5d7fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard to visualize training logs\n",
    "# Uncomment and run the following lines in a separate cell if using Jupyter Notebook\n",
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir=./logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ecd15d-8f47-47c1-8a0f-8427aa545f92",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we implemented and trained both DCGAN and CGAN models for image generation. The models were evaluated based on the quality of the generated images, demonstrating the effectiveness of GANs in producing realistic images. Further exploration could involve tuning hyperparameters, experimenting with different GAN architectures, or applying these models to more complex datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
